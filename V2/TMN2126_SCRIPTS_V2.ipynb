{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting RNA Half-Life from Epitranscriptomic and Structural Data using Convolutional Neural Networks\n"
      ],
      "metadata": {
        "id": "As39LLJdNxgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Samples**"
      ],
      "metadata": {
        "id": "c-MdxtjONywF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFD18FQgNph_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Year 4/Fall/Machine Learning for Functional Genomics/project/final/OSD-569_m6a_counts_munged.txt'\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_m6a = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated, change 'sep' accordingly if needed\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_m6a)"
      ],
      "metadata": {
        "id": "Ab01clHFNsDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Year 4/Fall/Machine Learning for Functional Genomics/project/final/OSD-569_transcriptome_counts_munged.txt'\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_transcripts = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated, change 'sep' accordingly if needed\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_transcripts)\n"
      ],
      "metadata": {
        "id": "1VBDl5jTQSQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Year 4/Fall/Machine Learning for Functional Genomics/project/final/OSD-569_sequences.txt'\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_sequences = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated, change 'sep' accordingly if needed\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_sequences)\n"
      ],
      "metadata": {
        "id": "anWZtfOdMCTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your text file\n",
        "file_path = '/content/drive/MyDrive/Year 4/Fall/Machine Learning for Functional Genomics/project/final/half_life.txt'\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_half_life = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated, change 'sep' accordingly if needed\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_half_life)\n"
      ],
      "metadata": {
        "id": "nQa1_E01daW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembl REST API Script - SEQUENCE CONTEXT IMPORT\n"
      ],
      "metadata": {
        "id": "5JoFaexxMKnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_transcript_sequence(ensembl_gene):\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "    ext = f\"/sequence/id/{ensembl_gene}?type=genomic\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"text/plain\",\n",
        "        \"Accept\": \"text/plain\"\n",
        "    }\n",
        "    response = requests.get(server + ext, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch transcript sequence for {ensembl_gene}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def extract_sequence_around_position(transcript_sequence, position):\n",
        "    if transcript_sequence:\n",
        "        position = int(position)\n",
        "        sequence_start = max(0, position - 25)\n",
        "        sequence_end = min(len(transcript_sequence), position + 25)\n",
        "        sequence_around_position = transcript_sequence[sequence_start:sequence_end]\n",
        "        return sequence_around_position\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Create a new column to store the sequences\n",
        "df_sequences['Genomic Sequence'] = ''\n",
        "\n",
        "# Remove numbers after the dot in ENSEMBL gene column\n",
        "df_sequences['ENSEMBL gene'] = df_sequences['ENSEMBL gene'].apply(lambda x: x.split('.')[0])\n",
        "\n",
        "# Loop through the DataFrame and fetch sequences\n",
        "for index, row in df_sequences.iterrows():\n",
        "    ensembl_gene = row['ENSEMBL gene']\n",
        "    position_gene = row['position gene']\n",
        "\n",
        "    # Fetch entire transcript sequence for the ENSEMBL gene ID\n",
        "    transcript_sequence = fetch_transcript_sequence(ensembl_gene)\n",
        "\n",
        "    if transcript_sequence:\n",
        "        # Extract sequence around the given position within the transcript sequence\n",
        "        sequence_around_position = extract_sequence_around_position(transcript_sequence, position_gene)\n",
        "        if sequence_around_position:\n",
        "            # Add the extracted sequence to the DataFrame\n",
        "            df_sequences.at[index, 'Genomic Sequence'] = sequence_around_position\n",
        "        else:\n",
        "            print(f\"Failed to extract sequence around position {position_gene} from the transcript sequence of {ensembl_gene}.\")\n",
        "    else:\n",
        "        print(f\"Transcript sequence not found for {ensembl_gene}.\")\n",
        "\n",
        "# Display the updated DataFrame with genomic sequences\n",
        "print(df_sequences)"
      ],
      "metadata": {
        "id": "E73chLR2QMcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export DataFrame to a tab-separated text file\n",
        "file_path = '/content/drive/MyDrive/Year 4/Fall/Machine Learning for Functional Genomics/project/final/OSD-569_sequences_filled.txt'\n",
        "df_sequences.to_csv(file_path, sep='\\t', index=False)"
      ],
      "metadata": {
        "id": "HxxHGBEOQ8jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_transcript_sequence(ensembl_gene):\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "    ext = f\"/sequence/id/{ensembl_gene}?type=cdna\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"text/plain\",\n",
        "        \"Accept\": \"text/plain\"\n",
        "    }\n",
        "    response = requests.get(server + ext, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch transcript sequence for {ensembl_gene}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def extract_sequence_around_position(transcript_sequence, position):\n",
        "    if transcript_sequence:\n",
        "        position = int(position)\n",
        "        sequence_start = 0\n",
        "        sequence_end = len(transcript_sequence)\n",
        "        sequence_around_position = transcript_sequence[sequence_start:sequence_end]\n",
        "        return sequence_around_position\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Specify the path to your text file\n",
        "file_path = '/Users/theo/Downloads/OSD-569_sequences.txt'\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_sequences = pd.read_csv(file_path, sep='\\t')  # Assuming tab-separated, change 'sep' accordingly if needed\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_sequences)\n",
        "\n",
        "# Create a new column to store the sequences\n",
        "df_sequences['Genomic Sequence'] = ''\n",
        "\n",
        "# Remove numbers after the dot in ENSEMBL gene column\n",
        "df_sequences['ENSEMBL transcript'] = df_sequences['ENSEMBL transcript'].apply(lambda x: x.split('.')[0])\n",
        "\n",
        "# Open the file to write sequences\n",
        "output_file_path = '/Users/theo/Downloads/OSD-569_sequences_complete.txt'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    sequence_counter = 0\n",
        "    save_interval = 100  # Define the interval to save the file\n",
        "    start_index = 20400  # Specify the index to start fetching sequences\n",
        "\n",
        "    # Loop through the DataFrame from the specified starting index\n",
        "    for index, row in df_sequences.iloc[start_index:].iterrows():\n",
        "        ensembl_gene = row['ENSEMBL transcript']\n",
        "        position_gene = row['position transcript']\n",
        "\n",
        "        # Fetch entire transcript sequence for the ENSEMBL gene ID\n",
        "        transcript_sequence = fetch_transcript_sequence(ensembl_gene)\n",
        "        print(index)\n",
        "        if transcript_sequence:\n",
        "            # Extract sequence around the given position within the transcript sequence\n",
        "            sequence_around_position = extract_sequence_around_position(transcript_sequence, position_gene)\n",
        "            if sequence_around_position:\n",
        "                # Write the sequence to the file immediately after fetching\n",
        "                output_file.write(f\"Sequence for index {index}:{sequence_around_position}\\n\")\n",
        "                sequence_counter += 1\n",
        "\n",
        "                # Save the file at regular intervals\n",
        "                if sequence_counter % save_interval == 0:\n",
        "                    output_file.flush()  # Flush buffer to write to file immediately\n",
        "                    print(f\"File saved after fetching {sequence_counter} sequences.\")\n",
        "            else:\n",
        "                print(f\"Failed to extract sequence around position {position_gene} from the transcript sequence of {ensembl_gene}.\")\n",
        "        else:\n",
        "            print(f\"Transcript sequence not found for {ensembl_gene}.\")\n",
        "\n",
        "# Confirm completion\n",
        "print(\"Sequences written to file.\")"
      ],
      "metadata": {
        "id": "GxRmnmE1rEn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_transcript_sequence(ensembl_gene):\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "    ext = f\"/sequence/id/{ensembl_gene}?type=cdna\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"text/plain\",\n",
        "        \"Accept\": \"text/plain\"\n",
        "    }\n",
        "    response = requests.get(server + ext, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.text\n",
        "    else:\n",
        "        print(f\"Failed to fetch transcript sequence for {ensembl_gene}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "def calculate_position_percentage(transcript_sequence, position):\n",
        "    if transcript_sequence:\n",
        "        total_length = len(transcript_sequence)\n",
        "        position_percentage = (int(position) / total_length) * 100\n",
        "        return position_percentage\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Specify the path to your text file\n",
        "file_path = '/Users/theo/Downloads/OSD-569_sequences.txt'  # Replace with your file path\n",
        "\n",
        "# Read the text file into a pandas DataFrame\n",
        "df_sequences = pd.read_csv(file_path, sep='\\t')  # Uncomment and adjust 'sep' accordingly if needed\n",
        "\n",
        "# Create a new column to store the position percentages\n",
        "df_sequences['Position Percentage'] = None\n",
        "\n",
        "# Remove numbers after the dot in ENSEMBL gene column\n",
        "df_sequences['ENSEMBL transcript'] = df_sequences['ENSEMBL transcript'].apply(lambda x: x.split('.')[0])\n",
        "\n",
        "# Output file path\n",
        "output_file_path = '/Users/theo/Downloads/OSD-569_positions_filled.txt'\n",
        "\n",
        "# Open the file to write sequences\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    sequence_counter = 0\n",
        "    save_interval = 100  # Define the interval to save the file\n",
        "    start_index = 0  # Specify the index to start fetching sequences\n",
        "\n",
        "    # Loop through the DataFrame from the specified starting index\n",
        "    for index, row in df_sequences.iloc[start_index:].iterrows():\n",
        "        ensembl_gene = row['ENSEMBL transcript']\n",
        "        position_gene = row['position transcript']\n",
        "\n",
        "        # Fetch entire transcript sequence for the ENSEMBL gene ID\n",
        "        transcript_sequence = fetch_transcript_sequence(ensembl_gene)\n",
        "        print(index)\n",
        "        if transcript_sequence:\n",
        "            # Calculate the position percentage\n",
        "            position_percentage = calculate_position_percentage(transcript_sequence, position_gene)\n",
        "            if position_percentage is not None:\n",
        "                # Write the percentage to the file immediately after calculating\n",
        "                output_file.write(f\"Percentage position for index {index}: {position_percentage}\\n\")\n",
        "                sequence_counter += 1\n",
        "\n",
        "                # Save the file at regular intervals\n",
        "                if sequence_counter % save_interval == 0:\n",
        "                    output_file.flush()  # Flush buffer to write to file immediately\n",
        "                    print(f\"File saved after processing {sequence_counter} entries.\")\n",
        "            else:\n",
        "                print(f\"Failed to calculate position percentage for {ensembl_gene} at position {position_gene}.\")\n",
        "        else:\n",
        "            print(f\"Transcript sequence not found for {ensembl_gene}.\")\n",
        "\n",
        "# Confirm completion\n",
        "output_file.flush()\n",
        "print(\"Position percentages written to file.\")"
      ],
      "metadata": {
        "id": "jUUYNNeOrIZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lasso Regression Script (m6a => gene expression prediction)\n"
      ],
      "metadata": {
        "id": "tcjo-ow8GBnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load m6A data\n",
        "m6a_data = df_m6a\n",
        "\n",
        "# Load transcriptomic data\n",
        "transcriptomic_data = df_transcripts\n",
        "\n",
        "# Replace 'NA' values with NaN for easier handling\n",
        "m6a_data.replace('NA', pd.NA, inplace=True)\n",
        "\n",
        "# Calculate average across columns C11 to C47 for each row\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "# Merge m6A data with transcriptomic data based on the common gene identifiers\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "\n",
        "# Select relevant columns for modeling\n",
        "features = merged_data[['m6a_avg']]\n",
        "target = merged_data[['C47_y']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n",
        "\n",
        "# Predict transcriptomic values for new data\n",
        "# Assuming 'new_m6a_data' contains new m6A values for prediction\n",
        "# new_m6a_data = pd.read_csv('path_to_new_m6a_data.csv')  # Replace 'path_to_new_m6a_data.csv' with your file path\n",
        "# new_m6a_data.replace('NA', pd.NA, inplace=True)\n",
        "# new_m6a_data['m6a_avg'] = new_m6a_data.iloc[:, 4:50].mean(axis=1)\n",
        "\n",
        "predicted_transcriptomic_values = model.predict(new_m6a_data[['m6a_avg']])"
      ],
      "metadata": {
        "id": "PjHv9raODVob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular Neural Network (m6a => gene expression prediction)\n"
      ],
      "metadata": {
        "id": "Yt7xkrWwGDTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Load m6A data\n",
        "m6a_data = df_m6a\n",
        "\n",
        "# Replace 'NA' values with NaN for easier handling\n",
        "m6a_data.replace('NA', pd.NA, inplace=True)\n",
        "\n",
        "# Average 'm6A_values' across columns C11 to C47\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values and positions)\n",
        "features = m6a_data[['m6a_avg']]\n",
        "\n",
        "# Load transcriptomic data\n",
        "transcriptomic_data = df_transcripts\n",
        "\n",
        "# Average 'Transcriptomic_value' across columns C11 to C47\n",
        "transcriptomic_data['Transcriptomic_avg'] = transcriptomic_data.iloc[:, 2:31].mean(axis=1)\n",
        "\n",
        "# Log normalization of 'Transcriptomic_avg' values\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data['Transcriptomic_avg'])\n",
        "\n",
        "# Merge m6A data with transcriptomic data based on the common gene identifiers\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "\n",
        "# Separate features and target\n",
        "X = features.values\n",
        "y = merged_data['Transcriptomic_avg_log'].values\n",
        "print(X)\n",
        "print(y)\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Get unique genes for train-test split\n",
        "unique_genes = merged_data['ENSEMBL gene'].unique()\n",
        "train_genes, test_genes = train_test_split(unique_genes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Filter data for train-test split based on gene\n",
        "train_data = merged_data[merged_data['ENSEMBL gene'].isin(train_genes)]\n",
        "test_data = merged_data[merged_data['ENSEMBL gene'].isin(test_genes)]\n",
        "\n",
        "# Train-test split\n",
        "X_train = X_scaled[train_data.index]\n",
        "y_train = train_data['Transcriptomic_avg_log'].values\n",
        "X_test = X_scaled[test_data.index]\n",
        "y_test = test_data['Transcriptomic_avg_log'].values\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "wFckWYIbGF81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Assuming the previous code for data preparation and model training has been executed\n",
        "\n",
        "# Generate ten random indices from the test set\n",
        "num_predictions = 10\n",
        "random_indices = np.random.choice(len(y_test), size=num_predictions, replace=False)\n",
        "\n",
        "# Get the actual values and corresponding predictions\n",
        "actual_values = y_test[random_indices]\n",
        "predicted_values = test_predictions[random_indices].flatten()\n",
        "\n",
        "# Calculate percentage error for each prediction\n",
        "percentage_errors = np.abs((predicted_values - actual_values) / actual_values) * 100\n",
        "\n",
        "# Create a DataFrame to display the information\n",
        "data = {\n",
        "    'Actual Values (Log Value)': actual_values,\n",
        "    'Predicted Values (Log Value)': predicted_values,\n",
        "    'Percentage Error (%)': percentage_errors\n",
        "}\n",
        "prediction_comparison = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(prediction_comparison)"
      ],
      "metadata": {
        "id": "g78NdOmw9NAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conventional Neural Network (m6a => gene expression prediction)"
      ],
      "metadata": {
        "id": "wy77E6mZHrtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Assuming df_sequences contains sequence data in the specified format\n",
        "# Let's consider 'sequence' column contains DNA sequences\n",
        "\n",
        "# Load merged data (contains m6a data)\n",
        "# Assuming merged_data contains the merged data of m6a with other features\n",
        "# Replace this with your actual merged data\n",
        "\n",
        "# Load transcriptomic data (Assuming df_transcriptomic contains transcriptomic data)\n",
        "# Replace this with your actual transcriptomic data\n",
        "# Load transcriptomic data\n",
        "transcriptomic_data = df_transcripts\n",
        "\n",
        "# Average 'Transcriptomic_value' across columns C11 to C47\n",
        "transcriptomic_data['Transcriptomic_avg'] = transcriptomic_data.iloc[:, 2:31].mean(axis=1)\n",
        "\n",
        "# Log normalization of 'Transcriptomic_avg' values\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data['Transcriptomic_avg'])\n",
        "\n",
        "# Merge m6A data with transcriptomic data based on the common gene identifiers\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "\n",
        "# Merge sequence data with existing merged data based on the common gene identifiers\n",
        "# Assuming 'ENSEMBL gene' is the common identifier between merged_data and df_sequences\n",
        "merged_data_with_sequence = pd.merge(merged_data, df_sequences, on=['ENSEMBL gene', 'position gene'], how='left')\n",
        "\n",
        "# Extracting Gene IDs without numbers after the dot\n",
        "merged_data_with_sequence['Gene_ID'] = merged_data_with_sequence['ENSEMBL gene'].str.split('.', expand=True)[0]\n",
        "\n",
        "# Merge the modified DataFrame with df_half_life based on the Gene ID\n",
        "merged_with_half_life = pd.merge(merged_data_with_sequence, df_half_life, left_on='Gene_ID', right_on='Ensembl Gene Id')\n",
        "\n",
        "# Use 'half-life (PC1)' as the target instead of transcriptomic outputs\n",
        "X = merged_data_with_sequence[['m6a_avg', 'sequences']]\n",
        "X['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values, positions, and sequence)\n",
        "features_with_sequence = merged_data_with_sequence[['m6a_avg', 'sequences']]\n",
        "\n",
        "# Handling missing sequence values (if any)\n",
        "features_with_sequence['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Define a mapping for one-hot encoding\n",
        "base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "# Define a function to perform one-hot encoding for DNA sequences\n",
        "def one_hot_encode_sequence(sequence, sequence_length):\n",
        "    encoded_sequence = np.zeros((sequence_length, 4))  # 4 for A, C, G, T\n",
        "    for i, base in enumerate(sequence[:sequence_length]):\n",
        "        if base in base_to_idx:\n",
        "            encoded_sequence[i, base_to_idx[base]] = 1\n",
        "    return encoded_sequence\n",
        "\n",
        "# Define the sequence length (adjust as needed based on your requirement)\n",
        "sequence_length = 50  # Assuming a sequence length of 100 bases\n",
        "\n",
        "# Apply one-hot encoding to the 'sequence' column\n",
        "features_with_sequence['one_hot_encoded_sequence'] = features_with_sequence['sequences'].apply(\n",
        "    lambda seq: one_hot_encode_sequence(seq, sequence_length)\n",
        ")\n",
        "\n",
        "# Prepare features (X) and target (y) for modeling\n",
        "X_sequence = np.array(features_with_sequence['one_hot_encoded_sequence'].tolist())\n",
        "X_m6a = features_with_sequence['m6a_avg'].values.reshape(-1, 1)\n",
        "X_sequence = X_sequence.reshape(X_sequence.shape[0], -1)\n",
        "X = np.concatenate((X_sequence, X_m6a), axis=1)\n",
        "\n",
        "# Extract transcriptomic target values\n",
        "y_transcriptomic = merged_data_with_sequence['Transcriptomic_avg_log'].values\n",
        "\n",
        "# Assuming 'Transcriptomic_avg_log' was the column name for transcriptomic values\n",
        "# Concatenate transcriptomic target values with existing y values\n",
        "y = np.concatenate((y, y_transcriptomic), axis=0)\n",
        "\n",
        "# Limit the number of data points to the first 15700\n",
        "X = X[:11000]\n",
        "y_transcriptomic = y_transcriptomic[:11000]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_transcriptomic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape)\n",
        "\n",
        "print(X_train_scaled.shape[0])\n",
        "\n",
        "# Assuming sequence_length and features are known\n",
        "sequence_length = 201\n",
        "features = 1\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(sequence_length, features)),  # Input shape adjusted for sequence data\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extract loss values from the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plotting training and validation loss across epochs\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "id": "8luFKwQoHr96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conventional Neural Network (m6a => mRNA decay)\n"
      ],
      "metadata": {
        "id": "sh_V5fChxjYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "KUGvhIcS5y5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Load m6A data\n",
        "m6a_data = df_m6a\n",
        "\n",
        "# Replace 'NA' values with NaN for easier handling\n",
        "m6a_data.replace('NA', pd.NA, inplace=True)\n",
        "\n",
        "# Average 'm6A_values' across columns C11 to C47\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values and positions)\n",
        "features = m6a_data[['m6a_avg']]\n",
        "\n",
        "# Load transcriptomic data\n",
        "transcriptomic_data = df_transcripts\n",
        "\n",
        "# Average 'Transcriptomic_value' across columns C11 to C47\n",
        "transcriptomic_data['Transcriptomic_avg'] = transcriptomic_data.iloc[:, 2:31].mean(axis=1)\n",
        "\n",
        "# Log normalization of 'Transcriptomic_avg' values\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data['Transcriptomic_avg'])\n",
        "\n",
        "# Merge m6A data with transcriptomic data based on the common gene identifiers\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "\n",
        "# Merge sequence data with existing merged data based on the common gene identifiers\n",
        "# Assuming 'ENSEMBL gene' is the common identifier between merged_data and df_sequences\n",
        "merged_data_with_sequence = pd.merge(merged_data, df_sequences, on=['ENSEMBL gene', 'position gene'], how='left')\n",
        "\n",
        "# Extracting Gene IDs without numbers after the dot\n",
        "merged_data_with_sequence['Gene_ID'] = merged_data_with_sequence['ENSEMBL gene'].str.split('.', expand=True)[0]\n",
        "\n",
        "# Merge the modified DataFrame with df_half_life based on the Gene ID\n",
        "merged_with_half_life = pd.merge(merged_data_with_sequence, df_half_life, left_on='Gene_ID', right_on='Ensembl Gene Id')\n",
        "\n",
        "# Use 'half-life (PC1)' as the target instead of transcriptomic outputs\n",
        "X = merged_with_half_life[['m6a_avg', 'sequences']]\n",
        "X['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Step 1: Filter Rows with More Than Twelve Non-NA m6A Values\n",
        "# Define the columns to check for non-NA m6a values\n",
        "m6a_columns = [\n",
        "    'C11_x', 'C12_x', 'C13_x', 'C14_x', 'C15_x', 'C16_x', 'C17_x',\n",
        "    'C21_x', 'C22_x', 'C23_x', 'C24_x', 'C25_x', 'C26_x', 'C27_x',\n",
        "    'C31_x', 'C32_x', 'C33_x', 'C34_x', 'C35_x', 'C36_x', 'C37_x',\n",
        "    'C41_x', 'C42_x', 'C43_x', 'C44_x', 'C45_x', 'C46_x', 'C47_x'\n",
        "]\n",
        "\n",
        "# Filter rows with more than twelve non-NA values\n",
        "merged_with_half_life_filtered = merged_with_half_life.dropna(subset=m6a_columns, thresh=12)\n",
        "\n",
        "# Step 2: Calculate Standard Deviation and Add as New Column\n",
        "merged_with_half_life_filtered['m6a_stddev'] = merged_with_half_life_filtered[m6a_columns].std(axis=1)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values, positions, and sequence)\n",
        "features_with_sequence = merged_with_half_life_filtered[['m6a_avg', 'm6a_stddev', 'sequences']]\n",
        "\n",
        "# Handling missing sequence values (if any)\n",
        "features_with_sequence['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Define a mapping for one-hot encoding\n",
        "base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "# Define a function to perform one-hot encoding for DNA sequences\n",
        "def one_hot_encode_sequence(sequence, sequence_length):\n",
        "    encoded_sequence = np.zeros((sequence_length, 4))  # 4 for A, C, G, T\n",
        "    for i, base in enumerate(sequence[:sequence_length]):\n",
        "        if base in base_to_idx:\n",
        "            encoded_sequence[i, base_to_idx[base]] = 1\n",
        "    return encoded_sequence\n",
        "\n",
        "# Define the sequence length (adjust as needed based on your requirement)\n",
        "sequence_length = 50  # Assuming a sequence length of 100 bases\n",
        "\n",
        "# Apply one-hot encoding to the 'sequence' column\n",
        "features_with_sequence['one_hot_encoded_sequence'] = features_with_sequence['sequences'].apply(\n",
        "    lambda seq: one_hot_encode_sequence(seq, sequence_length)\n",
        ")\n",
        "\n",
        "# Prepare features (X) and target (y) for modeling\n",
        "X_sequence = np.array(features_with_sequence['one_hot_encoded_sequence'].tolist())\n",
        "X_m6a = features_with_sequence['m6a_avg'].values.reshape(-1, 1)\n",
        "X_m6a_stddev = features_with_sequence['m6a_stddev'].values.reshape(-1, 1)\n",
        "X_sequence = X_sequence.reshape(X_sequence.shape[0], -1)\n",
        "X = np.concatenate((X_sequence, X_m6a, X_m6a_stddev), axis=1)\n",
        "\n",
        "# Extract transcriptomic target values\n",
        "y_transcriptomic = merged_with_half_life['half-life (PC1)'].values\n",
        "\n",
        "# Assuming 'Transcriptomic_avg_log' was the column name for transcriptomic values\n",
        "# Concatenate transcriptomic target values with existing y values\n",
        "y = np.concatenate((y, y_transcriptomic), axis=0)\n",
        "\n",
        "# Limit the number of data points to the first 2000\n",
        "X = X[:2000]\n",
        "y_transcriptomic = y_transcriptomic[:2000]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_transcriptomic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape)\n",
        "\n",
        "print(X_train_scaled.shape[0])\n",
        "\n",
        "# Assuming sequence_length and features are known\n",
        "sequence_length = 202\n",
        "features = 1\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(sequence_length, features)),  # Input shape adjusted for sequence data\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extract loss values from the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plotting training and validation loss across epochs\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "id": "THk-IMT4xk4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in merged_with_half_life.columns:\n",
        "    print(col)\n",
        "print(merged_with_half_life)"
      ],
      "metadata": {
        "id": "UUbLhLfcF-W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Assuming the previous code for data preparation and model training has been executed\n",
        "\n",
        "# Generate ten random indices from the test set\n",
        "num_predictions = 10\n",
        "random_indices = np.random.choice(len(y_test), size=num_predictions, replace=False)\n",
        "\n",
        "# Get the actual values and corresponding predictions\n",
        "actual_values = y_test[random_indices]\n",
        "predicted_values = test_predictions[random_indices].flatten()\n",
        "\n",
        "# Calculate percentage error for each prediction\n",
        "percentage_errors = np.abs((predicted_values - actual_values) / actual_values) * 100\n",
        "\n",
        "# Create a DataFrame to display the information\n",
        "data = {\n",
        "    'Actual Values (Log Value)': actual_values,\n",
        "    'Predicted Values (Log Value)': predicted_values,\n",
        "    'Percentage Error (%)': percentage_errors\n",
        "}\n",
        "prediction_comparison = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(prediction_comparison)\n",
        "\n",
        "# Calculate R-squared value\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "# Scatter plot of predicted vs actual values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot(y_test, y_test, color='red', label='Ideal Prediction')\n",
        "plt.title('Predicted vs Actual Values')\n",
        "plt.xlabel('Actual Values (Log Value)')\n",
        "plt.ylabel('Predicted Values (Log Value)')\n",
        "plt.legend()\n",
        "\n",
        "# Include R-squared value as text on the plot\n",
        "plt.text(0.1, 0.9, f'R-squared = {r_squared:.4f}', ha='center', va='center', transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZhiCB4o31bu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conventional Neural Network (m6a AVERAGE / STDDEV + POSITION => mRNA decay) V2\n"
      ],
      "metadata": {
        "id": "YnXLECP6zYD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "PAlduH46zdhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Load m6A data\n",
        "m6a_data = df_m6a\n",
        "\n",
        "# Replace 'NA' values with NaN for easier handling\n",
        "m6a_data.replace('NA', pd.NA, inplace=True)\n",
        "\n",
        "# Average 'm6A_values' across columns C11 to C47\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values and positions)\n",
        "features = m6a_data[['m6a_avg']]\n",
        "\n",
        "# Load transcriptomic data\n",
        "transcriptomic_data = df_transcripts\n",
        "\n",
        "# Average 'Transcriptomic_value' across columns C11 to C47\n",
        "transcriptomic_data['Transcriptomic_avg'] = transcriptomic_data.iloc[:, 2:31].mean(axis=1)\n",
        "\n",
        "# Log normalization of 'Transcriptomic_avg' values\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data['Transcriptomic_avg'])\n",
        "\n",
        "# Merge m6A data with transcriptomic data based on the common gene identifiers\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "\n",
        "# Merge sequence data with existing merged data based on the common gene identifiers\n",
        "# Assuming 'ENSEMBL gene' is the common identifier between merged_data and df_sequences\n",
        "merged_data_with_sequence = pd.merge(merged_data, df_sequences, on=['ENSEMBL gene', 'position gene'], how='left')\n",
        "\n",
        "# Extracting Gene IDs without numbers after the dot\n",
        "merged_data_with_sequence['Gene_ID'] = merged_data_with_sequence['ENSEMBL gene'].str.split('.', expand=True)[0]\n",
        "\n",
        "# Merge the modified DataFrame with df_half_life based on the Gene ID\n",
        "merged_with_half_life = pd.merge(merged_data_with_sequence, df_half_life, left_on='Gene_ID', right_on='Ensembl Gene Id')\n",
        "\n",
        "# Drop rows with NaN in the 'position' column\n",
        "merged_with_half_life = merged_with_half_life.dropna(subset=['position'])\n",
        "\n",
        "# Use 'half-life (PC1)' as the target instead of transcriptomic outputs\n",
        "X = merged_with_half_life[['m6a_avg', 'sequences', 'position']]\n",
        "X['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Step 1: Filter Rows with More Than Twelve Non-NA m6A Values\n",
        "# Define the columns to check for non-NA m6a values\n",
        "m6a_columns = [\n",
        "    'C11_x', 'C12_x', 'C13_x', 'C14_x', 'C15_x', 'C16_x', 'C17_x',\n",
        "    'C21_x', 'C22_x', 'C23_x', 'C24_x', 'C25_x', 'C26_x', 'C27_x',\n",
        "    'C31_x', 'C32_x', 'C33_x', 'C34_x', 'C35_x', 'C36_x', 'C37_x',\n",
        "    'C41_x', 'C42_x', 'C43_x', 'C44_x', 'C45_x', 'C46_x', 'C47_x'\n",
        "]\n",
        "\n",
        "# Filter rows with more than twelve non-NA values\n",
        "merged_with_half_life_filtered = merged_with_half_life.dropna(subset=m6a_columns, thresh=12)\n",
        "\n",
        "# Step 2: Calculate Standard Deviation and Add as New Column\n",
        "merged_with_half_life_filtered['m6a_stddev'] = merged_with_half_life_filtered[m6a_columns].std(axis=1)\n",
        "\n",
        "# Select relevant columns for modeling (averaged m6a values, positions, and sequence)\n",
        "features_with_sequence = merged_with_half_life[['m6a_avg', 'position', 'sequences']]\n",
        "\n",
        "# Handling missing sequence values (if any)\n",
        "features_with_sequence['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# Define a mapping for one-hot encoding\n",
        "base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "\n",
        "# Define a function to perform one-hot encoding for DNA sequences\n",
        "def one_hot_encode_sequence(sequence, sequence_length):\n",
        "    encoded_sequence = np.zeros((sequence_length, 4))  # 4 for A, C, G, T\n",
        "    for i, base in enumerate(sequence[:sequence_length]):\n",
        "        if base in base_to_idx:\n",
        "            encoded_sequence[i, base_to_idx[base]] = 1\n",
        "    return encoded_sequence\n",
        "\n",
        "# Define the sequence length (adjust as needed based on your requirement)\n",
        "sequence_length = 50  # Assuming a sequence length of 50 bases\n",
        "\n",
        "# Apply one-hot encoding to the 'sequence' column\n",
        "features_with_sequence['one_hot_encoded_sequence'] = features_with_sequence['sequences'].apply(\n",
        "    lambda seq: one_hot_encode_sequence(seq, sequence_length)\n",
        ")\n",
        "\n",
        "# Prepare features (X) and target (y) for modeling\n",
        "X_sequence = np.array(features_with_sequence['one_hot_encoded_sequence'].tolist())\n",
        "X_m6a = features_with_sequence['m6a_avg'].values.reshape(-1, 1)\n",
        "X_m6a_pos = features_with_sequence['position'].values.reshape(-1, 1)\n",
        "# X_m6a_stddev = features_with_sequence['m6a_stddev'].values.reshape(-1, 1)\n",
        "X_sequence = X_sequence.reshape(X_sequence.shape[0], -1)\n",
        "X = np.concatenate((X_sequence, X_m6a, X_m6a_pos), axis=1)\n",
        "\n",
        "# Extract transcriptomic target values\n",
        "y_transcriptomic = merged_with_half_life['half-life (PC1)'].values\n",
        "\n",
        "# Assuming 'Transcriptomic_avg_log' was the column name for transcriptomic values\n",
        "# Concatenate transcriptomic target values with existing y values\n",
        "# y = np.concatenate((y, y_transcriptomic), axis=0)\n",
        "\n",
        "# Limit the number of data points to the first 2000\n",
        "X = X[:4400]\n",
        "y_transcriptomic = y_transcriptomic[:4400]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_transcriptomic, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train_scaled.shape)\n",
        "\n",
        "print(X_train_scaled.shape[0])\n",
        "\n",
        "# Assuming sequence_length and features are known\n",
        "sequence_length = 202\n",
        "features = 1\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(sequence_length, features)),  # Input shape adjusted for sequence data\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),  # Adding dropout to prevent overfitting\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # Output layer\n",
        "])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Extract loss values from the training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plotting training and validation loss across epochs\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)"
      ],
      "metadata": {
        "id": "49a-oLAfziC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data\n",
        "m6a_data = df_m6a.replace('NA', pd.NA)\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "transcriptomic_data = df_transcripts\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data.iloc[:, 2:31].mean(axis=1))\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "merged_data = pd.merge(merged_data, df_sequences, on=['ENSEMBL gene', 'position gene'], how='left')\n",
        "merged_data['Gene_ID'] = merged_data['ENSEMBL gene'].str.split('.', expand=True)[0]\n",
        "merged_data = pd.merge(merged_data, df_half_life, left_on='Gene_ID', right_on='Ensembl Gene Id')\n",
        "\n",
        "# Drop rows with NaN in the 'position' column\n",
        "merged_data = merged_data.dropna(subset=['position'])\n",
        "\n",
        "# Prepare features and target\n",
        "features = merged_data[['m6a_avg', 'position', 'sequences']]\n",
        "features['sequences'].fillna('', inplace=True)\n",
        "\n",
        "# One-hot encoding for sequences\n",
        "base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "sequence_length = 50\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    encoded = np.zeros((sequence_length, 4))\n",
        "    for i, base in enumerate(sequence[:sequence_length]):\n",
        "        if base in base_to_idx:\n",
        "            encoded[i, base_to_idx[base]] = 1\n",
        "    return encoded\n",
        "\n",
        "features['encoded_sequences'] = features['sequences'].apply(one_hot_encode_sequence)\n",
        "\n",
        "# Prepare model input\n",
        "X_sequence = np.array(features['encoded_sequences'].tolist())\n",
        "X_m6a = features['m6a_avg'].values.reshape(-1, 1)\n",
        "X_position = features['position'].values.reshape(-1, 1)\n",
        "X = np.concatenate((X_sequence.reshape(X_sequence.shape[0], -1), X_m6a, X_position), axis=1)\n",
        "\n",
        "# Prepare target\n",
        "y = merged_data['half-life (PC1)'].values\n",
        "\n",
        "X = X[:50000]\n",
        "y = y[:50000]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1], 1)),\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(range(1, len(history.history['loss']) + 1), history.history['loss'], 'bo', label='Training Loss')\n",
        "plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], 'r', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "_5SUvE2wgqQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conventional Neural Network (m6a AVERAGE / STDDEV + POSITION => mRNA decay) V3"
      ],
      "metadata": {
        "id": "F8bpBBBprsrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1], 1)),\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n"
      ],
      "metadata": {
        "id": "j2eFFMV_Oopr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential Future CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1], 1)),\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "N_8oLmknQ6yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Conv1D, MaxPooling1D, Flatten, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data\n",
        "m6a_data = df_m6a.replace('NA', np.nan)\n",
        "m6a_data['m6a_avg'] = m6a_data.iloc[:, 4:33].mean(axis=1)\n",
        "\n",
        "transcriptomic_data = df_transcripts\n",
        "transcriptomic_data['Transcriptomic_avg_log'] = np.log1p(transcriptomic_data.iloc[:, 2:31].mean(axis=1))\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = pd.merge(m6a_data, transcriptomic_data, left_on='ENSEMBL gene', right_on='Genes')\n",
        "merged_data = pd.merge(merged_data, df_sequences, on=['ENSEMBL gene', 'position gene'], how='left')\n",
        "merged_data['Gene_ID'] = merged_data['ENSEMBL gene'].str.split('.', expand=True)[0]\n",
        "merged_data = pd.merge(merged_data, df_half_life, left_on='Gene_ID', right_on='Ensembl Gene Id')\n",
        "\n",
        "# Drop rows with NaN in the 'position' column\n",
        "merged_data = merged_data.dropna(subset=['position'])\n",
        "\n",
        "# Keep only the row with the largest 'position' value for each unique sequence in 'sequences_full'\n",
        "merged_data = merged_data.sort_values('position', ascending=False).drop_duplicates('sequences_full')\n",
        "\n",
        "# Define the columns to check for non-NA m6a values\n",
        "m6a_columns = [\n",
        "    'C11_x', 'C12_x', 'C13_x', 'C14_x', 'C15_x', 'C16_x', 'C17_x',\n",
        "    'C21_x', 'C22_x', 'C23_x', 'C24_x', 'C25_x', 'C26_x', 'C27_x',\n",
        "    'C31_x', 'C32_x', 'C33_x', 'C34_x', 'C35_x', 'C36_x', 'C37_x',\n",
        "    'C41_x', 'C42_x', 'C43_x', 'C44_x', 'C45_x', 'C46_x', 'C47_x'\n",
        "]\n",
        "\n",
        "# Calculate Standard Deviation and Add as New Column\n",
        "merged_data['m6a_stddev'] = merged_data[m6a_columns].std(axis=1)\n",
        "\n",
        "# Filter rows with 12 or more NaN values in m6a_columns\n",
        "# merged_data = merged_data.dropna(subset=m6a_columns, thresh=12)\n",
        "\n",
        "# Prepare features and target\n",
        "features = merged_data[['m6a_avg', 'position', 'sequences_full']] #\n",
        "features['sequences_full'].fillna('', inplace=True)\n",
        "\n",
        "# One-hot encoding for sequences\n",
        "base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
        "sequence_length = 3000\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    encoded = np.zeros((sequence_length, 4))\n",
        "    for i, base in enumerate(sequence[:sequence_length]):\n",
        "        if base in base_to_idx:\n",
        "            encoded[i, base_to_idx[base]] = 1\n",
        "    return encoded\n",
        "\n",
        "features['encoded_sequences'] = features['sequences_full'].apply(one_hot_encode_sequence)\n",
        "\n",
        "# Prepare model input\n",
        "X_sequence = np.array(features['encoded_sequences'].tolist())\n",
        "X_m6a = features['m6a_avg'].values.reshape(-1, 1)\n",
        "X_position = features['position'].values.reshape(-1, 1)\n",
        "X = np.concatenate((X_sequence.reshape(X_sequence.shape[0], -1), X_m6a, X_position), axis=1) # np.concatenate(, axis=1) # , X_m6a, X_position\n",
        "\n",
        "# Prepare target\n",
        "y = merged_data['half-life (PC1)'].values\n",
        "\n",
        "#y = merged_data['Transcriptomic_avg_log'].values\n",
        "\n",
        "X = X[:51245]\n",
        "y = y[:51245]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling features\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Continue with the rest of your model training and evaluation\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1], 1)),\n",
        "    Conv1D(32, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(128, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Conv1D(256, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(range(1, len(history.history['loss']) + 1), history.history['loss'], 'bo', label='Training Loss')\n",
        "plt.plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], 'r', label='Validation Loss')\n",
        "plt.title('mRNA Decay Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "XMJXFB87rb2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "# Assuming the previous code for data preparation and model training has been executed\n",
        "\n",
        "# Generate ten random indices from the test set\n",
        "num_predictions = 10\n",
        "random_indices = np.random.choice(len(y_test), size=num_predictions, replace=False)\n",
        "\n",
        "# Get the actual values and corresponding predictions\n",
        "actual_values = y_test[random_indices]\n",
        "predicted_values = test_predictions[random_indices].flatten()\n",
        "\n",
        "# Calculate percentage error for each prediction\n",
        "percentage_errors = np.abs((predicted_values - actual_values) / actual_values) * 100\n",
        "\n",
        "# Create a DataFrame to display the information\n",
        "data = {\n",
        "    'Actual Values (Log Value)': actual_values,\n",
        "    'Predicted Values (Log Value)': predicted_values,\n",
        "    'Percentage Error (%)': percentage_errors\n",
        "}\n",
        "prediction_comparison = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(prediction_comparison)\n",
        "\n",
        "# Calculate R-squared value\n",
        "r_squared = r2_score(y_test, test_predictions)\n",
        "\n",
        "# Scatter plot of predicted vs actual values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot(y_test, y_test, color='red', label='Ideal Prediction')\n",
        "plt.title('mRNA Decay Test Set Predictions')\n",
        "plt.xlabel('Actual Values (Log Value)')\n",
        "plt.ylabel('Predicted Values (Log Value)')\n",
        "plt.legend()\n",
        "\n",
        "# Include R-squared value as text on the plot\n",
        "plt.text(0.1, 0.9, f'R-squared = {r_squared:.4f}', ha='center', va='center', transform=plt.gca().transAxes, bbox=dict(facecolor='white', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qK3GiIxMgVAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V3 HELPER CODE"
      ],
      "metadata": {
        "id": "gMeLVvVbr2g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data['half-life (PC1)'])"
      ],
      "metadata": {
        "id": "k9tMOWztlS_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(set(merged_data['half-life (PC1)'])))"
      ],
      "metadata": {
        "id": "CdVmL0lyQAm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,100):\n",
        "  print(X_position[i])"
      ],
      "metadata": {
        "id": "EEGZvykVlYnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.isnan(X).any(), np.isnan(y_train).any())\n",
        "\n",
        "print(np.isinf(X).any(), np.isinf(y_train).any())"
      ],
      "metadata": {
        "id": "Q7JtabwChhLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Find the indices of NaN values\n",
        "nan_indices = np.where(np.isnan(X[:1000]))\n",
        "\n",
        "# Print the indices\n",
        "for row, col in zip(*nan_indices):\n",
        "    print(f\"NaN found at Row: {row}, Column: {col}\")\n",
        "    print(X[row])"
      ],
      "metadata": {
        "id": "FYCXPF6sjSs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Find the indices of NaN values\n",
        "nan_indices = np.where(np.isnan(X_train))\n",
        "\n",
        "# Print the indices\n",
        "for row, col in zip(*nan_indices):\n",
        "    print(f\"NaN found at Row: {row}, Column: {col}\")\n",
        "    print(X_train[row])"
      ],
      "metadata": {
        "id": "7fUo1iR_ht3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scripts for the Data / Data Table Properties / Distributions\n"
      ],
      "metadata": {
        "id": "8Ju4VrCTWIvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dimensions of the DataFrame\n",
        "print(f\"Number of rows: {df_m6a.shape[0]}\")\n",
        "print(f\"Number of columns: {df_m6a.shape[1]}\")\n",
        "print(f\"Dimensions: {df_m6a.shape}\")"
      ],
      "metadata": {
        "id": "5l7susrtWMxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows: {df_transcripts.shape[0]}\")\n",
        "print(f\"Number of columns: {df_transcripts.shape[1]}\")\n",
        "print(f\"Dimensions: {df_transcripts.shape}\")"
      ],
      "metadata": {
        "id": "yjiOqUyDWM7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Replace this with your actual DataFrame\n",
        "df = df_transcripts.iloc[:, 1:]\n",
        "\n",
        "# Plot cumulative probability curves for each column on the same graph with log scale on x-axis\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for column in df.columns:\n",
        "    values = df[column].dropna().sort_values()\n",
        "    cumulative_prob = np.arange(len(values)) / len(values)\n",
        "    plt.plot(values, cumulative_prob, label=column, marker='o', linestyle='-')\n",
        "\n",
        "plt.xscale('log')  # Set log scale for the x-axis\n",
        "plt.title('Cumulative Probability Curves for Each Sample (Log Scale)')\n",
        "plt.xlabel('Values (log scale)')\n",
        "plt.ylabel('Cumulative Probability')\n",
        "plt.ylim(0.7, 1.0)  # Set y-axis limits\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MtjFYcCRXpK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_m6a)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame is already loaded and named 'df'\n",
        "\n",
        "# Drop NaN values in the 'm6a_avg' column before plotting (if needed)\n",
        "m6a_avg_values = df_m6a['m6a_avg'].dropna()\n",
        "\n",
        "# Setting style\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Creating a distribution plot using seaborn\n",
        "sns.histplot(m6a_avg_values, kde=True, bins=30, color='skyblue', edgecolor='black')\n",
        "\n",
        "# Adding labels and title with improved font sizes\n",
        "plt.xlabel('m6a Averages', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.title('Distribution of Average m6a Values', fontsize=14)\n",
        "\n",
        "# Customize tick parameters\n",
        "plt.tick_params(axis='both', which='major', labelsize=10)\n",
        "\n",
        "# Show grid with dashed lines\n",
        "plt.grid(linestyle='--', linewidth=0.5)\n",
        "\n",
        "# Remove top and right spines\n",
        "sns.despine()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ROYMzOrC8Av8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_transcripts)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your DataFrame is named df_transcripts\n",
        "\n",
        "# Drop rows with zero Transcriptomic_avg if necessary\n",
        "df_filtered = df_transcripts[df_transcripts['Transcriptomic_avg'] != 0]\n",
        "\n",
        "# Log2 transform the 'Transcriptomic_avg' column\n",
        "df_filtered['Transcriptomic_avg_log2'] = np.log2(df_filtered['Transcriptomic_avg'] + 1)  # Adding 1 to avoid log of zero\n",
        "\n",
        "# Plotting the distribution using seaborn with log scales\n",
        "sns.set(style=\"whitegrid\")  # Setting the style for the plot\n",
        "plt.figure(figsize=(8, 6))  # Setting the figure size\n",
        "\n",
        "# Creating a histogram for the 'Transcriptomic_avg_log2' column with log scales\n",
        "sns.histplot(df_filtered['Transcriptomic_avg_log2'], kde=True, bins=30, color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Average Expression Log2 Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Log2 Transformed Expression Values')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MRtdKLy3_UVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'half-life (PC1)' is the column name in your DataFrame df_half_life\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Setting the figure size\n",
        "\n",
        "# Creating a histogram using Seaborn for 'half-life (PC1)' column\n",
        "sns.histplot(df_half_life['half-life (PC1)'], bins=30, color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Half-life')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Half-life Values')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bvYSJFmTeD2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming your DataFrame with the 'm6a_stddev' column is named merged_data\n",
        "\n",
        "# Optionally filter the DataFrame if necessary\n",
        "# For example, removing zero values if they don't make sense in your context\n",
        "# merged_data_filtered = merged_data[merged_data['m6a_stddev'] != 0]\n",
        "\n",
        "# If needed, you can apply a transformation to the data\n",
        "# For this example, I'll assume no transformation is applied\n",
        "\n",
        "# Plotting the distribution using seaborn\n",
        "sns.set(style=\"whitegrid\")  # Setting the style for the plot\n",
        "plt.figure(figsize=(8, 6))  # Setting the figure size\n",
        "\n",
        "# Creating a histogram for the 'm6a_stddev' column\n",
        "sns.histplot(merged_data['m6a_stddev'], kde=True, bins=30, color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Standard Deviation of m6A Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of m6A Standard Deviation')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M7NxLjUeQZAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of missing values for each column\n",
        "na_proportion = df_m6a.isna().mean()\n",
        "\n",
        "# Display the results\n",
        "print(\"Proportion of missing values in each column:\")\n",
        "print(na_proportion)"
      ],
      "metadata": {
        "id": "We_Pp3k9ZmxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of missing values for each column\n",
        "na_proportion = df_transcripts.isna().mean()\n",
        "\n",
        "# Display the results\n",
        "print(\"Proportion of missing values in each column:\")\n",
        "print(na_proportion)"
      ],
      "metadata": {
        "id": "B_dyaN0MZw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INTERIM REPORT CLEANING SCRIPT (DEPRECIATED)\n"
      ],
      "metadata": {
        "id": "TQkO9i0ulK-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with NaN values\n",
        "m6a_df_cleaned = m6a_df.dropna()\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(m6a_df)\n",
        "\n",
        "print(\"\\nDataFrame after removing rows with NaN values:\")\n",
        "print(m6a_df_cleaned)"
      ],
      "metadata": {
        "id": "DVdo8W-ZlDFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with any numerical value less than twenty\n",
        "df_transcripts_filtered = df_transcripts[(df >= 20).all(axis=1)]\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "print(df_transcripts)\n",
        "\n",
        "print(\"\\nDataFrame after removing rows with any numerical value less than twenty:\")\n",
        "print(df_transcripts_filtered)"
      ],
      "metadata": {
        "id": "rfHwlB3nnKOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INTERIM REPORT CNN (DEPRECIATED)\n"
      ],
      "metadata": {
        "id": "jB4AymrMQidF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "assert(torch.cuda.is_available()) # if this fails go to Runtime -> Change runtime type -> Set \"Hardware Accelerator\"\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "80PZ_7XLBZSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "assert(torch.cuda.is_available())\n",
        "device = \"cuda\""
      ],
      "metadata": {
        "id": "B3cCFtTAA492"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_m6a and df_transcripts are your DataFrames\n",
        "# Replace 'C11', 'C12', etc., with your actual column names\n",
        "\n",
        "# Selecting specific columns\n",
        "input_columns = ['C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17','C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27','C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37','C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47']  # List of input columns\n",
        "output_column = 'C11'\n",
        "\n",
        "# Randomly select 158 rows from m6a_df_cleaned\n",
        "random_rows = m6a_df_cleaned.sample(n=158, random_state=42)\n",
        "\n",
        "# Training data\n",
        "input_data_train = random_rows[input_columns].values\n",
        "output_data_train = df_transcripts_filtered[[output_column]].values\n",
        "\n",
        "# Testing data\n",
        "input_data_test = random_rows[input_columns].values\n",
        "output_data_test = df_transcripts_filtered[[output_column]].values\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "input_tensor_train = torch.tensor(input_data_train, dtype=torch.float32)\n",
        "output_tensor_train = torch.tensor(output_data_train, dtype=torch.float32).view(-1, 1)  # Reshape for single output\n",
        "\n",
        "input_tensor_test = torch.tensor(input_data_test, dtype=torch.float32)\n",
        "output_tensor_test = torch.tensor(output_data_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Print the sizes of tensors for debugging\n",
        "print(\"Size of input_tensor_train:\", input_tensor_train.size())\n",
        "print(\"Size of output_tensor_train:\", output_tensor_train.size())\n",
        "\n",
        "# Define the Deep Neural Network (DNN) model\n",
        "class DeepDNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super(DeepDNN, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.input_layer(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.relu(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "input_dim = len(input_columns)  # Number of input features\n",
        "hidden_dims = [8, 8, 8]  # Adjust the number of neurons in each hidden layer\n",
        "output_dim = 1\n",
        "\n",
        "model = DeepDNN(input_dim, hidden_dims, output_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(input_tensor_train, output_tensor_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(input_tensor_test)\n",
        "    test_loss = criterion(test_outputs, output_tensor_test)\n",
        "    print(f\"Test Loss: {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "D5sTDqHJMHjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_m6a and df_transcripts are your DataFrames\n",
        "# Replace 'C11', 'C12', etc., with your actual column names\n",
        "\n",
        "# Selecting specific columns\n",
        "input_columns = ['C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17','C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27','C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37','C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47']  # List of input columns\n",
        "output_columns = ['C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17','C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27','C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37','C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47']  # List of output columns\n",
        "\n",
        "# Randomly select 158 rows from m6a_df_cleaned\n",
        "random_rows = m6a_df_cleaned.sample(n=158, random_state=42)\n",
        "\n",
        "# Training data\n",
        "input_data_train = random_rows[input_columns].values\n",
        "output_data_train = df_transcripts_filtered[output_columns].values\n",
        "\n",
        "# Randomly select 158 rows from m6a_df_cleaned\n",
        "random_rows = m6a_df_cleaned.sample(n=158, random_state=3)\n",
        "\n",
        "# Testing data\n",
        "input_data_test = random_rows[input_columns].values\n",
        "output_data_test = df_transcripts_filtered[output_columns].values\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "input_tensor_train = torch.tensor(input_data_train, dtype=torch.float32)\n",
        "output_tensor_train = torch.tensor(output_data_train, dtype=torch.float32)\n",
        "\n",
        "input_tensor_test = torch.tensor(input_data_test, dtype=torch.float32)\n",
        "output_tensor_test = torch.tensor(output_data_test, dtype=torch.float32)\n",
        "\n",
        "# Print the sizes of tensors for debugging\n",
        "print(\"Size of input_tensor_train:\", input_tensor_train.size())\n",
        "print(\"Size of output_tensor_train:\", output_tensor_train.size())\n",
        "\n",
        "# Define the Deep Neural Network (DNN) model\n",
        "class DeepDNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
        "        super(DeepDNN, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])\n",
        "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.input_layer(x))\n",
        "        for layer in self.hidden_layers:\n",
        "            x = self.relu(layer(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "input_dim = len(input_columns)  # Number of input features\n",
        "hidden_dims = [128, 64, 1]  # Adjust the number of neurons in each hidden layer\n",
        "output_dim = len(output_columns)  # Number of output features\n",
        "\n",
        "model = DeepDNN(input_dim, hidden_dims, output_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(input_tensor_train, output_tensor_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Test the model on the test set\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_outputs = model(input_tensor_test)\n",
        "    test_loss = criterion(test_outputs, output_tensor_test)\n",
        "    print(f\"Test Loss: {test_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "IUBlOpm0soTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assuming df_m6a and df_transcripts are your DataFrames\n",
        "# Replace 'C11', 'C12', etc., with your actual column names\n",
        "\n",
        "# Selecting specific columns\n",
        "input_columns = ['C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47']  # List of input columns\n",
        "output_column = 'C11'\n",
        "\n",
        "# Randomly select 158 rows from df_m6a\n",
        "random_rows = m6a_df_cleaned.sample(n=158, random_state=42)\n",
        "\n",
        "# Log-normalize the input and output data\n",
        "log_normalized_train_data = np.log1p(random_rows[input_columns])\n",
        "log_normalized_train_labels = np.log1p(df_transcripts_filtered[output_column])\n",
        "\n",
        "random_rows = m6a_df_cleaned.sample(n=158, random_state=44)\n",
        "\n",
        "log_normalized_train_data = np.log1p(random_rows[input_columns])\n",
        "log_normalized_train_labels = np.log1p(df_transcripts_filtered[output_column])\n",
        "\n",
        "# Perform Lasso regression\n",
        "alpha = 0.0001 # Lasso regularization strength\n",
        "lasso_model = Lasso(alpha=alpha)\n",
        "lasso_model.fit(train_data, train_labels)\n",
        "\n",
        "# Make predictions on the test set\n",
        "lasso_predictions = lasso_model.predict(test_data)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(test_labels, lasso_predictions)\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "\n",
        "# Plotting the scatterplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatterplot of actual vs. predicted values\n",
        "plt.scatter(test_labels, lasso_predictions, color='blue', label='Actual vs. Predicted')\n",
        "\n",
        "# Plotting the diagonal line for reference\n",
        "plt.plot([min(test_labels), max(test_labels)], [min(test_labels), max(test_labels)], linestyle='--', color='red', label='Perfect Prediction')\n",
        "\n",
        "plt.title('Lasso Regression (Log-Normalized): Actual vs. Predicted')\n",
        "plt.xlabel('Log-Normalized Actual Values')\n",
        "plt.ylabel('Log-Normalized Predicted Values')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yT-GikML3ARa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}